 Measuring the Effectiveness of AI-Generated Test Suite in OAH Testing POC

Dear Leaders and Nirmal,

To rigorously assess the effectiveness of the AI-generated test suite in the OAH Testing POC, we are implementing a structured, data-driven evaluation framework focused on key quality metrics:

Key Metrics for Assessment
Requirement Coverage Rate – Percentage of business rules, edge cases, and data types covered.
Test Case Pass Rate – Proportion of test cases reaching the expected outcome in Kafka (Publish = Pass, Error = Fail).
Error Detection Rate – Identifies failure points and potential defects uncovered by AI-generated test cases.
Diversity & Relevance – Measures uniqueness, boundary conditions, and real-world applicability of test cases.
Clarity & Validity – Ensures test cases are well-structured, non-redundant, and actionable.
Implementation Approach
Tracking Table: Each test case is assigned a unique key to track its journey from source to Kafka topics (Publish/Error) while comparing expected vs. actual outcomes.
Requirement Coverage Matrix: Maps test cases to requirements, ensuring completeness.
Expert Review: Senior testers will rate test cases on clarity, detectability, and overall effectiveness.
Impact & Benefits
By combining quantifiable KPIs with expert validation, this approach will:
✔ Ensure comprehensive test coverage aligned with business needs.
✔ Validate the accuracy and reliability of AI-generated test cases.
✔ Identify potential gaps and failure points early.
✔ Improve overall test efficiency and defect detection capabilities.

This structured evaluation will provide clear insights into AI’s role in enhancing test effectiveness within our 6-week POC.

Best,
